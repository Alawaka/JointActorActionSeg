{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vision/u/jingweij/Softwares/anaconda2/envs/pad/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils.matterport_utils as matterport_utils\n",
    "from models.model_misc import compose_image_meta\n",
    "from utils.flow import read_flo_file\n",
    "from dataIO.a2d_dataset import A2DDataset\n",
    "from cfg.config import Config\n",
    "\n",
    "import os.path as osp\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import skimage\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = A2DDataset(split='train', dataset_dir='/vision/u/jingweij/Datasets/A2D/Release/')\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(dataset, video_id, frame_id, config, use_mini_mask=True):\n",
    "    \"\"\"Load and return ground truth data for an image (image, mask, bounding boxes).\n",
    "\n",
    "    augment: If true, apply random image augmentation. Currently, only\n",
    "        horizontal flipping is offered.\n",
    "    use_mini_mask: If False, returns full-size masks that are the same height\n",
    "        and width as the original image. These can be big, for example\n",
    "        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,\n",
    "        224x224 and are generated by extracting the bounding box of the\n",
    "        object and resizing it to MINI_MASK_SHAPE.\n",
    "\n",
    "    Returns:\n",
    "    image: [height, width, 3]\n",
    "    image_meta: meta info of image\n",
    "    rs_rgb_clip: [TIMESTEPS, height, width, 3], resized clip of rgb image\n",
    "    rs_flow_clip: [TIMESTEPS, height, width, 2], resized clip of flow\n",
    "    labeled_frame_id: [TIMESTEPS], one-hot vector indicating the location of this labeled frame in clip.\n",
    "    bbox: [instance_count, (y1, x1, y2, x2, actor_class_id, action_class_id)]\n",
    "    mask: [height, width, instance_count]. The height and width are those\n",
    "        of the image unless use_mini_mask is True, in which case they are\n",
    "        defined in MINI_MASK_SHAPE.\n",
    "    \"\"\"\n",
    "    image_path = osp.join(dataset.image_dir, video_id, '%05d.png' % frame_id)\n",
    "    flow_path = osp.join(dataset.flow_dir, video_id, '%05d.flo' % frame_id)\n",
    "    anno_path = osp.join(dataset.anno_dir, video_id, '%05d.png' % frame_id)\n",
    "    has_anno = osp.exists(anno_path)\n",
    "    # Load image and mask\n",
    "    image = load_image(image_path)\n",
    "    flow = load_flow(flow_path)\n",
    "    shape = image.shape\n",
    "    image, window, scale, padding = matterport_utils.resize_image(\n",
    "        image, \n",
    "        min_dim=config.IMAGE_MIN_DIM, \n",
    "        max_dim=config.IMAGE_MAX_DIM,\n",
    "        padding=config.IMAGE_PADDING)\n",
    "    # Active classes\n",
    "    # Different datasets have different classes, so track the\n",
    "    # classes supported in the dataset of this image.\n",
    "    active_actor_class_ids = np.ones([dataset.num_actor_classes], dtype=np.int32)\n",
    "    active_action_class_ids = np.ones([dataset.num_action_classes], dtype=np.int32)\n",
    "    # Image meta data. Here image_id is set -1, to be consistent with args in\n",
    "    # compose_image_meta, while distinct.\n",
    "    image_meta = compose_image_meta(-1, shape, window, active_actor_class_ids, active_action_class_ids)\n",
    "    \n",
    "    if not has_anno:\n",
    "        rtn_value = {\n",
    "            'image': image,\n",
    "            'flow': flow,\n",
    "            'image_path': image_path,\n",
    "            'image_meta': image_meta,\n",
    "        }\n",
    "        return rtn_value\n",
    "    \n",
    "    mask, actor_class_ids, action_class_ids = load_mask(dataset, anno_path)\n",
    "    mask = matterport_utils.resize_mask(mask, scale, padding)\n",
    "    # Bounding boxes. Note that some boxes might be all zeros\n",
    "    # if the corresponding mask got cropped out.\n",
    "    # bbox: [num_instances, (y1, x1, y2, x2)]\n",
    "    bbox = matterport_utils.extract_bboxes(mask)\n",
    "    # Add class_id as the last value in bbox\n",
    "    bbox = np.hstack([bbox, actor_class_ids[:, np.newaxis], action_class_ids[:, np.newaxis]])\n",
    "    # Resize masks to smaller size to reduce memory usage\n",
    "    if use_mini_mask:\n",
    "        mask = matterport_utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
    "\n",
    "    rtn_value = {\n",
    "        'image': image,\n",
    "        'flow': flow,\n",
    "        'image_path': image_path,\n",
    "        'image_meta': image_meta,\n",
    "        'mask': mask,\n",
    "        'bbox': bbox,\n",
    "    }\n",
    "    return rtn_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = skimage.io.imread(image_path)\n",
    "    # If grayscale. Convert to RGB for consistency.\n",
    "    if image.ndim != 3:\n",
    "        image = skimage.color.gray2rgb(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(dataset, anno_path):\n",
    "    anno = skimage.io.imread(anno_path).astype(np.int64)\n",
    "    anno = anno[:,:,0]*10**6 + anno[:,:,1]*10**3 + anno[:,:,2]\n",
    "    color_codes = np.unique(anno).tolist()[1:] # exclude 0 for background\n",
    "    num_instances = len(np.unique(anno)) - 1\n",
    "    assert num_instances == len(color_codes) # TODO: if assert passes, change to num_instances = len(color_codes)\n",
    "\n",
    "    mask = np.empty([anno.shape[0], anno.shape[1], num_instances])\n",
    "    for i in range(num_instances):\n",
    "        mask[:,:,i] = anno == color_codes[i]\n",
    "    actor_class_ids = np.array(list(map(lambda x: dataset.color_to_actor_class_name[x], color_codes)))\n",
    "    action_class_ids = np.array(list(map(lambda x: dataset.color_to_action_class_name[x], color_codes)))\n",
    "    return mask, actor_class_ids, action_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flow(flow_path):\n",
    "    return read_flo_file(flow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[64 64]\n",
      " [32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_MAX_DIM                  256\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [256 256   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.002\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           a2d\n",
      "NUM_ACTION_CLASSES             10\n",
      "NUM_ACTOR_CLASSES              8\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              2\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           128\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STPES               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class A2DConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"a2d\"\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_ACTOR_CLASSES = 1 + 7  # background + 7 actors\n",
    "    NUM_ACTION_CLASSES = 1 + 9  # background + 9 actions\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "    \n",
    "config = A2DConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_video_ids = sorted(os.listdir(dataset.image_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = np.random.choice(all_video_ids)\n",
    "frame_id = np.random.choice(len(os.listdir(osp.join(dataset.image_dir, video_id)))) + 1\n",
    "#video_id = 'tB2-L7hyGKo'\n",
    "#frame_id = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VZKTqH9gB0s', 211)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id, frame_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 650 ms, total: 1.88 s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    video_id = np.random.choice(all_video_ids)\n",
    "    frame_id = np.random.choice(len(os.listdir(osp.join(dataset.image_dir, video_id)))) + 1\n",
    "    dic = preprocess_image(dataset, video_id, frame_id, config, use_mini_mask=True)\n",
    "    with open('../tmp/%03d.pkl' % i, 'wb') as f:\n",
    "        pickle.dump(dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 284 ms, total: 413 ms\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "li = []\n",
    "for j in range(2):\n",
    "    for i in range(100):\n",
    "        with open('../tmp/%03d.pkl' % i, 'rb') as f:\n",
    "            li.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/vision/u/jingweij/Datasets/A2D/Release/Images/hsBHfZkpdAU/00002.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/FbVBQ6EstIc/00014.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/2xSLG0gm_og/00099.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/OuU52au3ts8/00060.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/YX1Ya8UA25c/00091.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/0RBrmdUTPkE/00236.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/3DQA-gR25aU/00009.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/MKkk74T94Os/00011.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/laAlYz7-TvI/00027.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/qeYwsddRGEY/00084.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/V2BmmY784So/00025.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Eg6obDE4juY/00195.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/eWoGfahOyJ4/00061.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/MgtfJ72akg8/00025.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/us3OFIQ4os0/00105.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/TRjZom1okjo/00070.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/LXMosPFgGgQ/00066.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/rPS54lgSunQ/00119.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/h8CxaQMSqkI/00063.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/h5afX9UjhUQ/00082.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/qIxRXG1uKyM/00113.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/vt8ZK8fI3W8/00067.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/I0zfxFTYM48/00116.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/f7QQKXPxjp0/00162.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/yKbUBxwOcyk/00014.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/lZDGZJH6Hnk/00039.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/kncSOPj6m4g/00029.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/-4CdF084nIs/00074.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/cjGbqaFdSMw/00100.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/4k56VzjQY2s/00122.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/rC8sCJS5R2s/00035.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/7V3EwEoPF7Q/00056.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/WdeQ5AfZXYE/00080.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/PG1kVHit-90/00133.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/9w5QxOYBEPI/00005.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/UcUmGzciPfo/00141.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/4wLoOodR3U0/00050.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/-lF5o3IOBm0/00014.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Jr0bXVlBd3A/00038.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/G_CZQVK3Pwk/00143.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/3z75155Qh0I/00053.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/wKPebmDivtc/00033.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/6Z2cBdleo2U/00032.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/20_siizMMEU/00018.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/dr_YGFzIW-M/00139.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/0deYp3CHiPw/00125.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/zPG5cvlfzgY/00096.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/7M_HU6Gtckk/00050.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/kdnl9RaeGvk/00039.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/c6knEYcFXVc/00023.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/M1blnLmHPB4/00078.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/PO-78KqoxtQ/00065.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/D6bFG54eGKA/00045.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/CqNITUm3WA8/00024.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/KTmhRDxJGXc/00052.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/HcWsk_1Pmrs/00011.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/UuCjxB662JY/00154.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/H_P_VvKC8Ds/00053.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Wy3SuhEQHVg/00080.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/CTAKpizCcx4/00027.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/7-1vLsnfU8o/00016.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/-esJrBWj2d8/00067.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/8roL132gA0s/00049.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/866n2kjpwT8/00079.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/dYjofylX3Cs/00054.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/HgqbCq_sxmo/00041.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/zSS2aKDVRco/00052.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Ki4arE8heOM/00036.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/5hZrRU67r78/00066.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ARSss_Z21uM/00046.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/gAnolgf38aU/00118.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/BO-qWTA5PMs/00012.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/EZA506VkA_0/00011.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ugOUpbWRTuY/00108.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/8Weonusl7aM/00006.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/2e8_DZ0z8HE/00222.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/j3_XpeMbjDc/00010.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/q4a6uiSO3hM/00027.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ss5A_oiodYk/00112.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/BkQxmOpSiRE/00066.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/vLgCJxMMlwE/00052.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/BhNE7bt5t8g/00113.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/GBqoZYfz4os/00031.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/wU8lymZO_VY/00052.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ZvgJ7mVxeg0/00030.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/p8nf_Bm9N54/00001.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/zcUQxkrHWbc/00020.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/VjwrqrG-ThI/00043.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/8BkbsIV2urQ/00123.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/uxNqb2dbuZk/00054.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Mf7OLAxjf5M/00085.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/30y8Uy0B_uk/00027.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/e3xk4ZqfrNA/00032.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/DCvnnSZCeCY/00010.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/KTmhRDxJGXc/00166.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/oICYKLYKgMg/00035.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/aB8Qj8ZVvJw/00106.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ih4LJ3e4Y2g/00083.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/GxfLe6LEvgI/00159.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/6zzZqXp0TMQ/00137.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/hsBHfZkpdAU/00002.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/FbVBQ6EstIc/00014.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/2xSLG0gm_og/00099.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/OuU52au3ts8/00060.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/YX1Ya8UA25c/00091.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/0RBrmdUTPkE/00236.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/3DQA-gR25aU/00009.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/MKkk74T94Os/00011.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/laAlYz7-TvI/00027.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/qeYwsddRGEY/00084.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/V2BmmY784So/00025.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Eg6obDE4juY/00195.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/eWoGfahOyJ4/00061.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/MgtfJ72akg8/00025.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/us3OFIQ4os0/00105.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/TRjZom1okjo/00070.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/LXMosPFgGgQ/00066.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/rPS54lgSunQ/00119.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/h8CxaQMSqkI/00063.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/h5afX9UjhUQ/00082.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/qIxRXG1uKyM/00113.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/vt8ZK8fI3W8/00067.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/I0zfxFTYM48/00116.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/f7QQKXPxjp0/00162.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/yKbUBxwOcyk/00014.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/lZDGZJH6Hnk/00039.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/kncSOPj6m4g/00029.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/-4CdF084nIs/00074.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/cjGbqaFdSMw/00100.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/4k56VzjQY2s/00122.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/rC8sCJS5R2s/00035.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/7V3EwEoPF7Q/00056.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/WdeQ5AfZXYE/00080.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/PG1kVHit-90/00133.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/9w5QxOYBEPI/00005.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/UcUmGzciPfo/00141.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/4wLoOodR3U0/00050.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/-lF5o3IOBm0/00014.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Jr0bXVlBd3A/00038.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/G_CZQVK3Pwk/00143.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/3z75155Qh0I/00053.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/wKPebmDivtc/00033.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/6Z2cBdleo2U/00032.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/20_siizMMEU/00018.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/dr_YGFzIW-M/00139.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/0deYp3CHiPw/00125.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/zPG5cvlfzgY/00096.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/7M_HU6Gtckk/00050.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/kdnl9RaeGvk/00039.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/c6knEYcFXVc/00023.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/M1blnLmHPB4/00078.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/PO-78KqoxtQ/00065.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/D6bFG54eGKA/00045.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/CqNITUm3WA8/00024.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/KTmhRDxJGXc/00052.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/HcWsk_1Pmrs/00011.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/UuCjxB662JY/00154.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/H_P_VvKC8Ds/00053.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Wy3SuhEQHVg/00080.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/CTAKpizCcx4/00027.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/7-1vLsnfU8o/00016.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/-esJrBWj2d8/00067.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/8roL132gA0s/00049.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/866n2kjpwT8/00079.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/dYjofylX3Cs/00054.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/HgqbCq_sxmo/00041.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/zSS2aKDVRco/00052.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Ki4arE8heOM/00036.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/5hZrRU67r78/00066.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ARSss_Z21uM/00046.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/gAnolgf38aU/00118.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/BO-qWTA5PMs/00012.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/EZA506VkA_0/00011.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ugOUpbWRTuY/00108.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/8Weonusl7aM/00006.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/2e8_DZ0z8HE/00222.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/j3_XpeMbjDc/00010.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/q4a6uiSO3hM/00027.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ss5A_oiodYk/00112.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/BkQxmOpSiRE/00066.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/vLgCJxMMlwE/00052.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/BhNE7bt5t8g/00113.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/GBqoZYfz4os/00031.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/wU8lymZO_VY/00052.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ZvgJ7mVxeg0/00030.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/p8nf_Bm9N54/00001.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/zcUQxkrHWbc/00020.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/VjwrqrG-ThI/00043.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/8BkbsIV2urQ/00123.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/uxNqb2dbuZk/00054.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/Mf7OLAxjf5M/00085.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/30y8Uy0B_uk/00027.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/e3xk4ZqfrNA/00032.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/DCvnnSZCeCY/00010.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/KTmhRDxJGXc/00166.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/oICYKLYKgMg/00035.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/aB8Qj8ZVvJw/00106.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/ih4LJ3e4Y2g/00083.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/GxfLe6LEvgI/00159.png',\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/6zzZqXp0TMQ/00137.png']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['image_path'] for x in li]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/OuU52au3ts8/00060.png',\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/cjGbqaFdSMw/00100.png',\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/7M_HU6Gtckk/00050.png',\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/CqNITUm3WA8/00024.png',\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/j3_XpeMbjDc/00010.png',\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/OuU52au3ts8/00060.png',\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/cjGbqaFdSMw/00100.png',\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/7M_HU6Gtckk/00050.png',\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/CqNITUm3WA8/00024.png',\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " '/vision/u/jingweij/Datasets/A2D/Release/Images/j3_XpeMbjDc/00010.png',\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['image_path'] if 'mask' in x else i for i, x in enumerate(li)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
